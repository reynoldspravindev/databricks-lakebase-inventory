{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8b6fd3-102f-4b1c-a58e-baf1a9a0d668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Exploratory Data Analysis (EDA) Summary**\n",
    "\n",
    "* The sample from [reynolds_apps_wksp_catalog.inventory_analytics.store_sales](#table) shows that `warehouse_id`, `category_id`, and `item_name` are well-populated, with a diverse set of values for each feature.\n",
    "* The label column `ss_quantity` contains some nulls, which will need to be imputed or removed during preprocessing.\n",
    "* There is a wide range of item names, indicating high cardinality for this categorical feature. One-hot encoding or target encoding may be required.\n",
    "* Next steps:\n",
    "  * Impute missing values in `ss_quantity`.\n",
    "  * Encode categorical features, especially `item_name`.\n",
    "  * Prepare the dataset for train/validation/test split.\n",
    "\n",
    "Proceeding to feature preprocessing and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2981145-f636-405c-be54-f681a277d6f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocess features and label"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load relevant columns from the table, sample 100,000 rows for pandas processing\n",
    "df = spark.table('reynolds_apps_wksp_catalog.inventory_analytics.store_sales') \\\n",
    "    .select('warehouse_id', 'category_id', 'item_name', 'ss_quantity', 'date') \\\n",
    "    .sample(False, 100000/164427736, seed=42)\n",
    "\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Extract month from date\n",
    "pdf['month'] = pd.to_datetime(pdf['date']).dt.month\n",
    "\n",
    "# Impute missing values in ss_quantity with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "pdf['ss_quantity'] = imputer.fit_transform(pdf[['ss_quantity']])\n",
    "pdf['ss_quantity'] = pdf['ss_quantity'].astype(int)\n",
    "\n",
    "# One-hot encode categorical features including month\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(pdf[['warehouse_id', 'category_id', 'item_name', 'month']])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['warehouse_id', 'category_id', 'item_name', 'month']))\n",
    "\n",
    "# Combine features and label\n",
    "final_df = pd.concat([pdf[['warehouse_id', 'category_id', 'item_name', 'month', 'ss_quantity']], encoded_df], axis=1)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30f9dca-fb44-4930-adb0-89fabd24d190",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split data for modeling"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the preprocessed pandas DataFrame 'final_df'\n",
    "raw_feature_cols = ['warehouse_id', 'category_id', 'item_name', 'month']\n",
    "X = final_df[raw_feature_cols]\n",
    "y = final_df['ss_quantity']\n",
    "\n",
    "# First split into train (60%) and temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# Then split temp into validation (20%) and test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train set count: {len(X_train)}\")\n",
    "print(f\"Validation set count: {len(X_val)}\")\n",
    "print(f\"Test set count: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae2c600-ecdd-41ae-acf4-2c68eecafdf5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create an UC Volume"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS reynolds_apps_wksp_catalog.inventory_analytics.mlflow_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54f71e0-7169-4db5-9660-fcc5cc6e2da6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train and tune regression model"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "feature_cols = ['warehouse_id', 'category_id', 'item_name', 'month']\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_enc = encoder.fit_transform(X_train[feature_cols])\n",
    "X_val_enc = encoder.transform(X_val[feature_cols])\n",
    "X_test_enc = encoder.transform(X_test[feature_cols])\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "model = rf.fit(X_train_enc, y_train)\n",
    "val_pred = model.predict(X_val_enc)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "test_pred = model.predict(X_test_enc)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "import mlflow.pyfunc\n",
    "class DemandForecastingModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model, encoder, feature_cols, train_df):\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        self.feature_cols = feature_cols\n",
    "        self.train_df = train_df\n",
    "    def predict(self, context, model_input):\n",
    "        X = pd.DataFrame(model_input)[self.feature_cols]\n",
    "        # Fallback logic for missing months\n",
    "        preds = []\n",
    "        for _, row in X.iterrows():\n",
    "            enc = self.encoder.transform([row.values])\n",
    "            try:\n",
    "                pred = self.model.predict(enc)[0]\n",
    "            except Exception:\n",
    "                # Fallback: average for warehouse/category/month\n",
    "                mask = (self.train_df['warehouse_id'] == row['warehouse_id']) & (self.train_df['category_id'] == row['category_id']) & (self.train_df['month'] == row['month'])\n",
    "                if mask.any():\n",
    "                    pred = self.train_df.loc[mask, 'ss_quantity'].mean()\n",
    "                else:\n",
    "                    # Fallback: average for warehouse/category\n",
    "                    mask2 = (self.train_df['warehouse_id'] == row['warehouse_id']) & (self.train_df['category_id'] == row['category_id'])\n",
    "                    pred = self.train_df.loc[mask2, 'ss_quantity'].mean()\n",
    "            preds.append(pred)\n",
    "        return np.array(preds)\n",
    "input_example = X_train[feature_cols].iloc[:5]\n",
    "signature = infer_signature(X_train[feature_cols], model.predict(X_train_enc))\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=DemandForecastingModel(model, encoder, feature_cols, pd.concat([X_train, y_train], axis=1)),\n",
    "        input_example=input_example,\n",
    "        signature=signature\n",
    "    )\n",
    "    mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor + OneHotEncoder (pyfunc) + month\")\n",
    "    mlflow.log_param(\"features\", \", \".join(feature_cols))\n",
    "    mlflow.log_param(\"label\", \"ss_quantity\")\n",
    "    print(\"Model logged to MLflow as pyfunc with month feature and fallback logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c744d0ec-3c1e-46fc-a1c7-e9c637221e25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train and tune regression model"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "feature_cols = ['warehouse_id', 'category_id', 'item_name', 'month']\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_enc = encoder.fit_transform(X_train[feature_cols])\n",
    "X_val_enc = encoder.transform(X_val[feature_cols])\n",
    "X_test_enc = encoder.transform(X_test[feature_cols])\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "model = rf.fit(X_train_enc, y_train)\n",
    "val_pred = model.predict(X_val_enc)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "test_pred = model.predict(X_test_enc)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "import mlflow.pyfunc\n",
    "class DemandForecastingModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model, encoder, feature_cols, train_df):\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        self.feature_cols = feature_cols\n",
    "        self.train_df = train_df\n",
    "    def predict(self, context, model_input):\n",
    "        X = pd.DataFrame(model_input)[self.feature_cols]\n",
    "        preds = []\n",
    "        for _, row in X.iterrows():\n",
    "            enc = self.encoder.transform([row.values])\n",
    "            try:\n",
    "                pred = self.model.predict(enc)[0]\n",
    "            except Exception:\n",
    "                mask = (self.train_df['warehouse_id'] == row['warehouse_id']) & (self.train_df['category_id'] == row['category_id']) & (self.train_df['month'] == row['month'])\n",
    "                if mask.any():\n",
    "                    pred = self.train_df.loc[mask, 'ss_quantity'].mean()\n",
    "                else:\n",
    "                    mask2 = (self.train_df['warehouse_id'] == row['warehouse_id']) & (self.train_df['category_id'] == row['category_id'])\n",
    "                    pred = self.train_df.loc[mask2, 'ss_quantity'].mean()\n",
    "            preds.append(pred)\n",
    "        return np.array(preds)\n",
    "# Fix: Cast integer columns to float in input_example for MLflow signature\n",
    "input_example = X_train[feature_cols].iloc[:5].copy()\n",
    "for col in ['warehouse_id', 'category_id', 'month']:\n",
    "    input_example[col] = input_example[col].astype(float)\n",
    "signature = infer_signature(input_example, model.predict(X_train_enc[:5]))\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=DemandForecastingModel(model, encoder, feature_cols, pd.concat([X_train, y_train], axis=1)),\n",
    "        input_example=input_example,\n",
    "        signature=signature\n",
    "    )\n",
    "    mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor + OneHotEncoder (pyfunc) + month\")\n",
    "    mlflow.log_param(\"features\", \", \".join(feature_cols))\n",
    "    mlflow.log_param(\"label\", \"ss_quantity\")\n",
    "    print(\"Model logged to MLflow as pyfunc with month feature and fallback logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aaf1a84-2a75-42db-89e2-205535d8c013",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use SDK To Get Use Auth Token"
    }
   },
   "outputs": [],
   "source": [
    "def get_workspace_url_and_headers():\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "    w = WorkspaceClient()\n",
    "    headers = w.config.authenticate()\n",
    "    return workspace_url, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8517c81-3b36-4ba1-a03d-356b90baaf8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register MLflow model and create serving endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Set MLflow registry URI to Unity Catalog BEFORE any client/experiment code\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/reynolds_apps_wksp_catalog/inventory_analytics/mlflow_vol\"\n",
    "\n",
    "def get_workspace_url_and_headers():\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "    w = WorkspaceClient()\n",
    "    headers = w.config.authenticate()\n",
    "    return workspace_url, headers\n",
    "\n",
    "workspace_url, headers = get_workspace_url_and_headers()\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(mlflow.get_experiment(mlflow.active_run().info.experiment_id).name) if mlflow.active_run() else None\n",
    "experiment_id = experiment.experiment_id if experiment else None\n",
    "\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id] if experiment_id else None, order_by=[\"start_time DESC\"], max_results=20)\n",
    "run_id = None\n",
    "for _, row in runs.iterrows():\n",
    "    artifacts = client.list_artifacts(row['run_id'])\n",
    "    if any(a.path == 'model' for a in artifacts):\n",
    "        run_id = row['run_id']\n",
    "        break\n",
    "if not run_id:\n",
    "    raise RuntimeError(\"No run with a logged model artifact found.\")\n",
    "print(f\"Using MLflow run ID: {run_id}\")\n",
    "\n",
    "# Register as Unity Catalog model\n",
    "uc_model_name = \"reynolds_apps_wksp_catalog.inventory_analytics.demand_forecasting_model\"\n",
    "result = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/model\",\n",
    "    name=uc_model_name\n",
    ")\n",
    "print(f\"Registered Unity Catalog model version: {result.version}\")\n",
    "\n",
    "# Wait for model to be READY\n",
    "for _ in range(20):\n",
    "    model_version_details = client.get_model_version(uc_model_name, result.version)\n",
    "    status = model_version_details.status\n",
    "    print(f\"Model version status: {status}\")\n",
    "    if status == \"READY\":\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "# Update the serving endpoint to use the new Unity Catalog model version\n",
    "endpoint_name = \"demand-forecasting-endpoint\"\n",
    "payload = {\n",
    "    \"name\": endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"model_name\": uc_model_name,\n",
    "                \"model_version\": str(result.version),\n",
    "                \"workload_type\": \"CPU\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "endpoint_url = f\"https://{workspace_url}/api/2.0/serving-endpoints/{endpoint_name}/config\"\n",
    "response = requests.patch(endpoint_url, headers=headers, json=payload[\"config\"])\n",
    "if response.status_code == 404:\n",
    "    response = requests.post(\n",
    "        f\"https://{workspace_url}/api/2.0/serving-endpoints\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5cd943-3e75-4ad1-98f2-2d8dcff0e3e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Print model serving endpoint URL"
    }
   },
   "outputs": [],
   "source": [
    "# Print the model serving endpoint URL\n",
    "print(f\"Model Serving Endpoint URL: https://{workspace_url}/serving-endpoints/{endpoint_name}/invocations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d44217-6b23-4a98-9c1a-f709ddb0e55f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check model serving endpoint status"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check endpoint status\n",
    "status_url = f\"https://{workspace_url}/api/2.0/serving-endpoints/{endpoint_name}\"\n",
    "response = requests.get(status_url, headers=headers)\n",
    "status_info = response.json()\n",
    "\n",
    "# Print status and readiness\n",
    "print(\"Model Serving Endpoint Status\")\n",
    "# print(status_info)\n",
    "\n",
    "while (status_info.get(\"state\", {}).get(\"config_update\", False)) == 'IN_PROGRESS':\n",
    "    response = requests.get(status_url, headers=headers)\n",
    "    \n",
    "    status_info = response.json()\n",
    "\n",
    "    print(str(datetime.now()),\"Current status:\", (status_info.get(\"state\", {}).get(\"config_update\", False)))\n",
    "    \n",
    "    time.sleep(120)\n",
    "\n",
    "final_status = str(status_info.get(\"state\", {}).get(\"ready\"))\n",
    "if final_status == 'READY':\n",
    "    print(f\"Endpoint is {final_status} for inference.\")\n",
    "else:\n",
    "    print(status_info)\n",
    "    raise(\"Endpoint is not ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988917be-20b7-40ab-b580-08b080de90ec",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761172163463}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Batch inference using model serving endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Example: Prepare a batch of input data (replace with your actual data)\n",
    "base_batch = pd.DataFrame([\n",
    "    {\"warehouse_id\": 1, \"category_id\": 9, \"item_name\": \"Gaming Controller\"},\n",
    "    {\"warehouse_id\": 4, \"category_id\": 5, \"item_name\": \"Bestselling Novel\"}\n",
    "])\n",
    "\n",
    "# Add month feature for current and next two months\n",
    "current_month = datetime.now().month\n",
    "months = [(current_month + i ) % 12 + 1 for i in range(3)]\n",
    "\n",
    "batch_data = pd.DataFrame([\n",
    "    {**row, \"month\": m}\n",
    "    for _, row in base_batch.iterrows()\n",
    "    for m in months\n",
    "])\n",
    "\n",
    "# Ensure input types match model signature (float for ids/month)\n",
    "for col in [\"warehouse_id\", \"category_id\", \"month\"]:\n",
    "    batch_data[col] = batch_data[col].astype(float)\n",
    "\n",
    "# Prepare the request payload with raw features\n",
    "payload = {\"dataframe_split\": {\n",
    "    \"columns\": batch_data.columns.tolist(),\n",
    "    \"data\": batch_data.values.tolist()\n",
    "}}\n",
    "\n",
    "# print(\"Payload being sent to endpoint:\")\n",
    "# print(payload)\n",
    "\n",
    "# Call the serving endpoint\n",
    "response = requests.post(\n",
    "    f\"https://{workspace_url}/serving-endpoints/{endpoint_name}/invocations\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# print(\"Raw response from endpoint:\")\n",
    "# print(response.text)\n",
    "\n",
    "# Convert the predictions to a pandas DataFrame and include input columns\n",
    "try:\n",
    "    result = response.json()\n",
    "    if isinstance(result, list):\n",
    "        predictions_df = pd.DataFrame(result, columns=[\"prediction\"])\n",
    "    elif isinstance(result, dict) and \"predictions\" in result:\n",
    "        predictions_df = pd.DataFrame(result[\"predictions\"], columns=[\"prediction\"])\n",
    "    else:\n",
    "        predictions_df = pd.DataFrame([result])\n",
    "    if \"prediction\" in predictions_df.columns:\n",
    "        predictions_df[\"prediction\"] = predictions_df[\"prediction\"].round().astype(int)\n",
    "    output_df = pd.concat([batch_data.reset_index(drop=True), predictions_df.reset_index(drop=True)], axis=1)\n",
    "    print(\"Batch inference response as DataFrame (with input columns):\")\n",
    "    display(output_df)\n",
    "except Exception as e:\n",
    "    print(\"Failed to parse response as DataFrame. Raw response:\")\n",
    "    print(response.text)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4745389501912959,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DemandForecasting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}