{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0e7e0b8-83a0-454f-890e-f28d54d49b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../0 - SETUP/0 - Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2961faa0-35b0-4b42-9701-e0cbce472454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Create The Sales Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01a383b-4567-4d92-b664-dcec1292c7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lakebase_catalog_name = CATALOG_NAME\n",
    "catalog_name = ANALYTICS_CATALOG_NAME\n",
    "\n",
    "lakebase_schema_name= POSTGRES_SCHEMA\n",
    "schema_name = ANALYTICS_SCHEMA_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1775d186-4b4f-46ed-9caf-048b7b6e6a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f'USE CATALOG {catalog_name}')\n",
    "\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce556c9c-821b-4fa4-a779-c5d278b72b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Generate synthetic sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75eb768-4be0-4eff-8c21-68cf0c24c277",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763500866159}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get distinct warehouse_id, category_id, sku_id combinations from inventory_items\n",
    "items_df = spark.sql(f\"\"\"\n",
    "SELECT DISTINCT warehouse_id, ii.sku_id, category_id\n",
    "FROM {lakebase_catalog_name}.{lakebase_schema_name}.inventory_items ii\n",
    "JOIN {lakebase_catalog_name}.{lakebase_schema_name}.inventory_sku isk\n",
    "ON ii.sku_id = isk.sku_id\n",
    "\"\"\")\n",
    "\n",
    "# Generate date-hour range for past 3 years till yesterday\n",
    "start_date = datetime.now() - timedelta(days=3*365)\n",
    "end_date = datetime.now() - timedelta(days=1)\n",
    "total_hours = int((end_date - start_date).total_seconds() // 3600) + 1\n",
    "datetime_list = [(start_date + timedelta(hours=x)).replace(minute=0, second=0, microsecond=0) for x in range(total_hours)]\n",
    "datetimes_df = spark.createDataFrame([(dt,) for dt in datetime_list], [\"transaction_ts\"])\n",
    "\n",
    "# Cross join items_with_category_df with datetimes_df\n",
    "sales_base_df = items_df.crossJoin(datetimes_df)\n",
    "\n",
    "# Add random quantity (1-100)\n",
    "sales_df = sales_base_df.withColumn(\"quantity\", F.expr(\"CAST(FLOOR(rand() * 100) + 1 AS INT)\"))\\\n",
    "    .withColumn(\"date\", F.date_format(F.col(\"transaction_ts\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Select and order columns as requested\n",
    "synthetic_sales_df = sales_df.select(\n",
    "   \"transaction_ts\", \"date\",  \"warehouse_id\", \"category_id\", \"sku_id\", \"quantity\", \n",
    ")\n",
    "\n",
    "display(synthetic_sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ead333b-4fa2-4f54-9674-997e8f7aef05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Save the data as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e6ae4c-b582-4ea5-b5cb-3adb4f1b7f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.store_sales\")\n",
    "\n",
    "synthetic_sales_df.write.saveAsTable(f\"{catalog_name}.{schema_name}.store_sales\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd27c96c-4069-4f90-9990-b6ec534d59c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Next Steps\n",
    "- Since synthetic data generation is completed. Proceed to the DemandForecasting notebook "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7527783275802258,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2.1 - Generate Synthetic Sales Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
