{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b8b6fd3-102f-4b1c-a58e-baf1a9a0d668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demand Forecasting Model Development Notebook\n",
    "\n",
    "This notebook demonstrates the end-to-end process of building, training, and deploying a demand forecasting model using Databricks, Spark, scikit-learn, and MLflow with Unity Catalog integration.\n",
    "\n",
    "A RandomForest regressor is used for the forecasting.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Setup and Initialization**\n",
    "   * Load required libraries and set up Spark and MLflow environments.\n",
    "   * Select the appropriate Unity Catalog and schema for data access.\n",
    "\n",
    "2. **Data Loading and Exploration**\n",
    "   * Load a sample of the `store_sales` table from Unity Catalog.\n",
    "   * Perform exploratory data analysis (EDA) to understand feature distributions and missing values.\n",
    "\n",
    "3. **Feature Engineering and Preprocessing**\n",
    "   * Aggregate sales data by `warehouse_id`, `category_id`, `sku_id`, and `date`.\n",
    "   * Extract the month from the date for seasonality features.\n",
    "   * Impute missing values in the label column (`ss_quantity`) using the mean.\n",
    "   * One-hot encode categorical features: `warehouse_id`, `category_id`, `sku_id`, and `month`.\n",
    "\n",
    "4. **Data Splitting**\n",
    "   * Split the processed data into training, validation, and test sets for model development and evaluation.\n",
    "\n",
    "5. **Model Training and Evaluation**\n",
    "   * Train a Random Forest Regressor on the training set.\n",
    "   * Evaluate model performance using RMSE on validation and test sets.\n",
    "\n",
    "6. **Model Packaging and Logging**\n",
    "   * Wrap the trained model and encoder in a custom MLflow pyfunc model with fallback logic for unseen categories.\n",
    "   * Log the model, metrics, and parameters to MLflow with Unity Catalog tracking.\n",
    "\n",
    "7. **Model Registration and Deployment**\n",
    "   * Register the trained model as a Unity Catalog model.\n",
    "   * Create or update a Databricks Model Serving endpoint to serve the registered model.\n",
    "\n",
    "8. **Endpoint Validation**\n",
    "   * Print the serving endpoint URL.\n",
    "   * Check the status of the serving endpoint to ensure readiness for inference.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a reproducible workflow for demand forecasting, including robust data preprocessing, model training, and production deployment using Databricks best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c02649-0727-4f43-b141-8a86881daf32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../0 - SETUP/0 - Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2981145-f636-405c-be54-f681a277d6f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preprocess features and label"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load relevant columns from the table, sample 100,000 rows for pandas processing\n",
    "df = spark.table(f'{ANALYTICS_CATALOG_NAME}.{ANALYTICS_SCHEMA_NAME}.store_sales') \\\n",
    "    .select('warehouse_id', 'category_id', 'sku_id', 'quantity', 'date') \\\n",
    "    .groupBy('warehouse_id', 'category_id', 'sku_id', 'date').agg(F.sum('quantity').alias('ss_quantity'))\\\n",
    "    .sample(False, 100000/164427736, seed=42)\n",
    "\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Extract month from date\n",
    "pdf['month'] = pd.to_datetime(pdf['date']).dt.month\n",
    "\n",
    "# Impute missing values in ss_quantity with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "pdf['ss_quantity'] = imputer.fit_transform(pdf[['ss_quantity']])\n",
    "pdf['ss_quantity'] = pdf['ss_quantity'].astype(int)\n",
    "\n",
    "# One-hot encode categorical features including month\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(pdf[['warehouse_id', 'category_id', 'sku_id', 'month']])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['warehouse_id', 'category_id', 'sku_id', 'month']))\n",
    "\n",
    "# Combine features and label\n",
    "final_df = pd.concat([pdf[['warehouse_id', 'category_id', 'sku_id', 'month', 'ss_quantity']], encoded_df], axis=1)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30f9dca-fb44-4930-adb0-89fabd24d190",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split data for modeling"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the preprocessed pandas DataFrame 'final_df'\n",
    "raw_feature_cols = ['warehouse_id', 'category_id', 'sku_id', 'month']\n",
    "X = final_df[raw_feature_cols]\n",
    "y = final_df['ss_quantity']\n",
    "\n",
    "# First split into train (60%) and temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# Then split temp into validation (20%) and test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train set count: {len(X_train)}\")\n",
    "print(f\"Validation set count: {len(X_val)}\")\n",
    "print(f\"Test set count: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae2c600-ecdd-41ae-acf4-2c68eecafdf5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create an UC Volume"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {ANALYTICS_CATALOG_NAME}.{ANALYTICS_SCHEMA_NAME}.mlflow_vol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c744d0ec-3c1e-46fc-a1c7-e9c637221e25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train and tune regression model"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "feature_cols = ['warehouse_id', 'category_id', 'sku_id', 'month']\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_enc = encoder.fit_transform(X_train[feature_cols])\n",
    "X_val_enc = encoder.transform(X_val[feature_cols])\n",
    "X_test_enc = encoder.transform(X_test[feature_cols])\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "model = rf.fit(X_train_enc, y_train)\n",
    "val_pred = model.predict(X_val_enc)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "test_pred = model.predict(X_test_enc)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "import mlflow.pyfunc\n",
    "class DemandForecastingModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model, encoder, feature_cols, train_df):\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        self.feature_cols = feature_cols\n",
    "        self.train_df = train_df\n",
    "    def predict(self, context, model_input):\n",
    "        X = pd.DataFrame(model_input)[self.feature_cols]\n",
    "        preds = []\n",
    "        for _, row in X.iterrows():\n",
    "            enc = self.encoder.transform([row.values])\n",
    "            try:\n",
    "                pred = self.model.predict(enc)[0]\n",
    "            except Exception:\n",
    "                mask = (self.train_df['warehouse_id'] == row['warehouse_id']) & (self.train_df['category_id'] == row['category_id']) & (self.train_df['month'] == row['month'])\n",
    "                if mask.any():\n",
    "                    pred = self.train_df.loc[mask, 'ss_quantity'].mean()\n",
    "                else:\n",
    "                    mask2 = (self.train_df['warehouse_id'] == row['warehouse_id']) & (self.train_df['category_id'] == row['category_id'])\n",
    "                    pred = self.train_df.loc[mask2, 'ss_quantity'].mean()\n",
    "            preds.append(pred)\n",
    "        return np.array(preds)\n",
    "# Fix: Cast integer columns to float in input_example for MLflow signature\n",
    "input_example = X_train[feature_cols].iloc[:5].copy()\n",
    "for col in ['warehouse_id', 'category_id', 'month']:\n",
    "    input_example[col] = input_example[col].astype(float)\n",
    "signature = infer_signature(input_example, model.predict(X_train_enc[:5]))\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=DemandForecastingModel(model, encoder, feature_cols, pd.concat([X_train, y_train], axis=1)),\n",
    "        input_example=input_example,\n",
    "        signature=signature\n",
    "    )\n",
    "    mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor + OneHotEncoder (pyfunc) + month\")\n",
    "    mlflow.log_param(\"features\", \", \".join(feature_cols))\n",
    "    mlflow.log_param(\"label\", \"ss_quantity\")\n",
    "    print(\"Model logged to MLflow as pyfunc with month feature and fallback logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aaf1a84-2a75-42db-89e2-205535d8c013",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use SDK To Get Use Auth Token"
    }
   },
   "outputs": [],
   "source": [
    "def get_workspace_url_and_headers():\n",
    "    workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "    headers = w.config.authenticate()\n",
    "    return workspace_url, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8517c81-3b36-4ba1-a03d-356b90baaf8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register MLflow model and create serving endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Set MLflow registry URI to Unity Catalog BEFORE any client/experiment code\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = f\"/Volumes/{ANALYTICS_CATALOG_NAME}/{ANALYTICS_SCHEMA_NAME}/mlflow_vol\"\n",
    "\n",
    "workspace_url, headers = get_workspace_url_and_headers()\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(mlflow.get_experiment(mlflow.active_run().info.experiment_id).name) if mlflow.active_run() else None\n",
    "experiment_id = experiment.experiment_id if experiment else None\n",
    "\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id] if experiment_id else None, order_by=[\"start_time DESC\"], max_results=20)\n",
    "run_id = None\n",
    "for _, row in runs.iterrows():\n",
    "    artifacts = client.list_artifacts(row['run_id'])\n",
    "    if any(a.path == 'model' for a in artifacts):\n",
    "        run_id = row['run_id']\n",
    "        break\n",
    "if not run_id:\n",
    "    raise RuntimeError(\"No run with a logged model artifact found.\")\n",
    "print(f\"Using MLflow run ID: {run_id}\")\n",
    "\n",
    "# Register as Unity Catalog model\n",
    "uc_model_name = f\"{ANALYTICS_CATALOG_NAME}.{ANALYTICS_SCHEMA_NAME}.demand_forecasting_model\"\n",
    "result = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/model\",\n",
    "    name=uc_model_name\n",
    ")\n",
    "print(f\"Registered Unity Catalog model version: {result.version}\")\n",
    "\n",
    "# Wait for model to be READY\n",
    "for _ in range(20):\n",
    "    model_version_details = client.get_model_version(uc_model_name, result.version)\n",
    "    status = model_version_details.status\n",
    "    print(f\"Model version status: {status}\")\n",
    "    if status == \"READY\":\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "# Update the serving endpoint to use the new Unity Catalog model version\n",
    "endpoint_name = \"demand-forecasting-endpoint\"\n",
    "payload = {\n",
    "    \"name\": endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"model_name\": uc_model_name,\n",
    "                \"model_version\": str(result.version),\n",
    "                \"workload_type\": \"CPU\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "endpoint_url = f\"https://{workspace_url}/api/2.0/serving-endpoints/{endpoint_name}/config\"\n",
    "response = requests.patch(endpoint_url, headers=headers, json=payload[\"config\"])\n",
    "if response.status_code == 404:\n",
    "    response = requests.post(\n",
    "        f\"https://{workspace_url}/api/2.0/serving-endpoints\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5cd943-3e75-4ad1-98f2-2d8dcff0e3e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Print model serving endpoint URL"
    }
   },
   "outputs": [],
   "source": [
    "# Print the model serving endpoint URL\n",
    "print(f\"Model Serving Endpoint URL: https://{workspace_url}/serving-endpoints/{endpoint_name}/invocations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d44217-6b23-4a98-9c1a-f709ddb0e55f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check model serving endpoint status"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Check endpoint status\n",
    "status_url = f\"https://{workspace_url}/api/2.0/serving-endpoints/{endpoint_name}\"\n",
    "response = requests.get(status_url, headers=headers)\n",
    "status_info = response.json()\n",
    "\n",
    "# Print status and readiness\n",
    "print(\"Model Serving Endpoint Status\")\n",
    "# print(status_info)\n",
    "\n",
    "while (status_info.get(\"state\", {}).get(\"config_update\", False)) == 'IN_PROGRESS':\n",
    "    response = requests.get(status_url, headers=headers)\n",
    "    \n",
    "    status_info = response.json()\n",
    "\n",
    "    print(str(datetime.now()),\"Current status:\", (status_info.get(\"state\", {}).get(\"config_update\", False)))\n",
    "    \n",
    "    time.sleep(120)\n",
    "\n",
    "final_status = str(status_info.get(\"state\", {}).get(\"ready\"))\n",
    "if final_status == 'READY':\n",
    "    print(f\"Endpoint is {final_status} for inference.\")\n",
    "else:\n",
    "    print(status_info)\n",
    "    raise(\"Endpoint is not ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988917be-20b7-40ab-b580-08b080de90ec",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761172163463}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "OPTIONAL - Test Batch inference using model serving endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Example: Prepare a batch of input data (replace with your actual data)\n",
    "base_batch = pd.DataFrame([\n",
    "    {\"warehouse_id\": 1, \"category_id\": 9, \"sku_id\": 43},\n",
    "    {\"warehouse_id\": 4, \"category_id\": 5, \"sku_id\": 21}\n",
    "])\n",
    "\n",
    "# Add month feature for current and next two months\n",
    "current_month = datetime.now().month\n",
    "months = [(current_month + i ) % 12 + 1 for i in range(3)]\n",
    "\n",
    "batch_data = pd.DataFrame([\n",
    "    {**row, \"month\": m}\n",
    "    for _, row in base_batch.iterrows()\n",
    "    for m in months\n",
    "])\n",
    "\n",
    "# Ensure input types match model signature (float for ids/month)\n",
    "for col in [\"warehouse_id\", \"category_id\", \"sku_id\", \"month\"]:\n",
    "    batch_data[col] = batch_data[col].astype(float)\n",
    "\n",
    "# Prepare the request payload with raw features\n",
    "payload = {\"dataframe_split\": {\n",
    "    \"columns\": batch_data.columns.tolist(),\n",
    "    \"data\": batch_data.values.tolist()\n",
    "}}\n",
    "\n",
    "# print(\"Payload being sent to endpoint:\")\n",
    "# print(payload)\n",
    "\n",
    "# Call the serving endpoint\n",
    "response = requests.post(\n",
    "    f\"https://{workspace_url}/serving-endpoints/{endpoint_name}/invocations\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# print(\"Raw response from endpoint:\")\n",
    "# print(response.text)\n",
    "\n",
    "# Convert the predictions to a pandas DataFrame and include input columns\n",
    "try:\n",
    "    result = response.json()\n",
    "    if isinstance(result, list):\n",
    "        predictions_df = pd.DataFrame(result, columns=[\"prediction\"])\n",
    "    elif isinstance(result, dict) and \"predictions\" in result:\n",
    "        predictions_df = pd.DataFrame(result[\"predictions\"], columns=[\"prediction\"])\n",
    "    else:\n",
    "        predictions_df = pd.DataFrame([result])\n",
    "    if \"prediction\" in predictions_df.columns:\n",
    "        predictions_df[\"prediction\"] = predictions_df[\"prediction\"].round().astype(int)\n",
    "    output_df = pd.concat([batch_data.reset_index(drop=True), predictions_df.reset_index(drop=True)], axis=1)\n",
    "    print(\"Batch inference response as DataFrame (with input columns):\")\n",
    "    display(output_df)\n",
    "except Exception as e:\n",
    "    print(\"Failed to parse response as DataFrame. Raw response:\")\n",
    "    print(response.text)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d6fac88-86de-4c39-9a26-a235567b4ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Next Steps:\n",
    "- Navigate to the serving endpoint and grant **\"Can Query\" **permissions to the Databricks Inventory App's Service Principal Id. The App's Service Principal Id can be obtained from the App's **'Authorization'** page.\n",
    "- _Redeploy_ the Databricks app from the UI or programatically (CLI/REST/SDK)\n",
    "- Check how Real-time inferencing works using the the **\"AI suggestion\"** feature on app's _add item_ feature."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7527783275802237,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.2 - Demand Forecasting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
